{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Perceptron as Perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "During this lab, we will write our own perceptron and try to train it to output some interesting things. It will demonstrate the basics of machine learning algorithms in today's understanding. \n",
    "\n",
    "For a single perceptron we keep the notation:\n",
    "- the input vectors:\n",
    "    \n",
    "    $\\vec{X} \\equiv (x_1, ... , x_d) \\in \\mathbb{R} ^ d$\n",
    "\n",
    "- the weights acting on the input vector:\n",
    "\n",
    "    $\\vec{W} \\equiv (w_1, ... , w_d) \\in \\mathbb{R} ^ d$\n",
    "\n",
    "The perceptron acts like a basic structure of a [neuron](https://qbi.uq.edu.au/brain/brain-anatomy/what-neuron). In the simplest case it has:\n",
    "\n",
    "- dendrites (many) - receiving part of the neurons that receive synaptic input - those are our input vectors\n",
    "- a cell body - weights of the neuron $\\vec{W}$\n",
    "- single axon - output transmitting action potential excitations - here the output of activation function $\\phi (x_1, ..., x_d)$\n",
    "\n",
    "The simplest activation function for the classification algorithm has been shown during the lecture and is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi (\\vec{X}) = \n",
    "\\left \\{\n",
    "   \\begin{array}{lr}\n",
    "       1&\\sum_ j w_j x_j \\geq \\text{threshold} \\\\\n",
    "       0&\\text{otherwise}\n",
    "   \\end{array}\n",
    "\\right .\n",
    "\\end{equation}\n",
    "\n",
    "In this scenario we imagine that based on some input values (for example the temperature, humidity etc.) we decide on some binary decision. Any biggest decision can be made of those binary decisions. If one decision is more important than the second one we can write $\\text{threshold} = -b$, where $b$ is the bias and we can include it in the perceptron algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our $\\vec{X} = (1, x_1, ..., x_d)$ and $\\vec{W} = (b, w_1, ..., w_d)$, where we have included the $0-th$ dimension and our problem becomes $d+1$ dimensional one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Given is a perceptron with predefined weights:\n",
    "- $ w _0 = b = 2 $ (bias),\n",
    "- $ w_1 = 1$,\n",
    "- $ w_2 = 1$\n",
    "\n",
    "a) What is the dimension of the original input vector of the perceptron?\n",
    "\n",
    "b) Write the weights as a numpy vector and check what is the output for vectors for range of $[-2, 2]$ for each of the dimensions. What separation does this perceptron represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct your own perceptron class.\n",
    "It should know has the field for the dimension and have the setters and getters accordingly. It should also implement the multiplication of the dot product. For now, keep the activation function to be defined as before. Test the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2711373341.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [270]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_b(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    # Initialize the bias and the dimension of the perceptron. \n",
    "    # Initialize the weights to be zero (for now at least)\n",
    "    def __init__(self, W : np.array,  b = 0):\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "    \n",
    "    # Create getters for the perceptron.\n",
    "    def get_N(self):\n",
    "\n",
    "    def get_b(self):\n",
    "\n",
    "    def get_W(self):\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "\n",
    "    def set_b(self, b):\n",
    "\n",
    "    def set_W(self, W : np.array):\n",
    "\n",
    "    \n",
    "    ############################## GETTERS ##############################\n",
    "    \n",
    "    # override __getitem__ method to obtain the weight at a certain position\n",
    "    # remember that you need also to override __setitem__, __getslice__\n",
    "\n",
    "    \n",
    "    # set the string output of the perceptron \n",
    "    def __str__(self):\n",
    "\n",
    "    ############################## OPERATORS OVERRIDE ##############################\n",
    "     \n",
    "    # override multiplication operators\n",
    "    def __mul__(self, other):\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "\n",
    "    # implement the net_output of signals -> multiplication (weighted sum)\n",
    "    def net_output(self, X):\n",
    "\n",
    "    \n",
    "    # implement the activation function\n",
    "    def activation_function(self, X):\n",
    "\n",
    "   \n",
    "    def predict(self, X):\n",
    "        return self.activation_function(X)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Perceptron(np.array([0,1,2]), 5)\n",
    "b = np.array([[1,2,3], [-11,-11,-1]])\n",
    "a.predict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the appropriate weights for the perceptron to reproduce this table:\n",
    "\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "    \\text{Training example} & x_1 & x_2 & \\text{Classification} \\\\\n",
    "    A&0&1&0\\\\\n",
    "    B&2&0&0\\\\\n",
    "    C&1&1&1\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3428465396.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [267]\u001b[1;36m\u001b[0m\n\u001b[1;33m    a =\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a =\n",
    "a.predict(np.array([[0,1], [2,0], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a program that uses perceptrons implemented above to realize the binary gates:\n",
    "\n",
    "- [NAND](https://en.wikipedia.org/wiki/NAND_gate)\n",
    "- NOT\n",
    "- [AND](https://en.wikipedia.org/wiki/AND_gate)\n",
    "- [OR](https://en.wikipedia.org/wiki/OR_gate)\n",
    "- [XOR](https://en.wikipedia.org/wiki/XOR_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[1.,0],[0,1.],[1.,1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOT\n",
    "$$ w_1 * 0 + b >= 0 $$\n",
    "$$ w_1 * 1 + b < 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND\n",
    "p_not = lambda x : \n",
    "p_not([[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND\n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b >= 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND\n",
    "\n",
    "p_and = lambda x : \n",
    "p_and(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR \n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b >= 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b >= 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b >= 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR\n",
    "p_or = lambda x : \n",
    "p_or(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR\n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b >= 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b >= 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b < 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XOR\n",
    "\n",
    "p_xor = lambda x : Perceptron(np.array([-0.5, 0.5]), 3) * x\n",
    "p_xor(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that XOR cannot be so easily implemented. We need to combine previous outputs. Let us demonstrate that on NAND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAND\n",
    "p_nand = lambda x: p_not(p_and(x))\n",
    "p_nand(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_xor = \n",
    "p_xor(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build Logic class that has all the methods above put together.\n",
    "1. Use it to build a [Half-adder](https://www.geeksforgeeks.org/half-adder-in-digital-logic/) (hint: you may implement combination of outputs to be an imput to other array).\n",
    "2. Implement Half-adder using NAND gates only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all together \n",
    "\n",
    "\n",
    "\n",
    "class Logic:\n",
    "    \n",
    "    # combine the outputs of two gates to be an input to another gate again\n",
    "    def Comb(out1, out2):\n",
    "        return np.concatenate([out1, out2], axis = 1)\n",
    "    # take the input of the left side only\n",
    "    def Left(x):\n",
    "        return x[:,0].reshape(len(x),1)\n",
    "    # take the input of the right side only\n",
    "    def Right(x):\n",
    "        return x[:,1].reshape(len(x),1)\n",
    "    \n",
    "    ########################### BASIC LOGIC ###########################\n",
    "    def Not(x):\n",
    "        return \n",
    "    def Or(x):\n",
    "        return \n",
    "    def And(x):\n",
    "        return \n",
    "    def Nand(x):\n",
    "        return \n",
    "    def Xor(x):\n",
    "        return \n",
    "    # implement the half_adder\n",
    "    def H_A(x):\n",
    "        return \n",
    "    # implement the half_adder using NAND only\n",
    "    def H_A_N(x):\n",
    "        nand1 = \n",
    "        nand2 = \n",
    "        nand3 = \n",
    "        suma = \n",
    "        carry = \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]]),\n",
       " array([[0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]),\n",
       " array([[0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Logic.And(X), Logic.Or(X), Logic.Xor(X), Logic.H_A(X), Logic.H_A_N(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perceptron training\n",
    "The training in perceptron algorithm is an easy optimization task. In order to do it, we have to divide the dataset that we have into training, and testing. Sometimes the training part is also divided into training and validation. \n",
    "\n",
    "Let $\\{(\\vec{X_i}, y_i), i = 1, ... , N\\}$, be a training dataset (each pair of vector $X_i$ and it's classification $y_i$ is a training instance). The prediction of the perceptron is denoted as $$ \\hat{y_i} = \\phi(\\vec{W}\\cdot \\vec{X_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include the bias vector in $\\vec{W}$ directly or exclude it as before. $\\Phi$ is again the activation function.\n",
    "\n",
    "In general, the goal of `training` is to find the most optimal parameters to ensure that our prediction $\\hat{y_i}$ is as close to $y_i$ as possible (up to some metric at least). \n",
    "\n",
    "Such metric can be for example an [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance): $$ ||y_i - \\hat{y_i}||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the goal is to minimize that distance with respect to all training instances and feature-label pairs. The combination of all distances together is called a `loss function`. This is a very important concept as different optimization problems can have different loss functions. For example, a common loss function for regression is `mean squared error`:\n",
    "$$ L_{MSE} = \\frac{1}{N} \\sum _i ^N (y_i - \\hat{y}_i )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask how to train the mapping to find the optimal weights in $\\vec{W}$. The training is an `iterative` process, where in each step we adjust the weights and biases to be closer and closer to the `extremum`. \n",
    "\n",
    "- we only want a small change at each step not to jump around like madmen\n",
    "- in order to find the direction to minimum, we can go against the direction of the biggest slope -> `the gradient`. This means that our activation function needs to be differentiable everywhere!\n",
    "- one of possibilities is to use [Gradient descent](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) algorithm:\n",
    "\n",
    "\n",
    "Gradient Descent Algorithm iteratively calculates the next point using gradient at the current position, scales it (by a learning rate) and subtracts obtained value from the current position (makes a step). It subtracts the value because we want to minimise the function (to maximise it would be adding). This process can be written as:\n",
    "$$ p_{n+1} = p_n - \\eta \\nabla f(p_n)$$\n",
    "\n",
    "- The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point\n",
    "- If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely.\n",
    "\n",
    "In summary, Gradient Descent methodâ€™s steps are:\n",
    "\n",
    "1. choose a starting point (initialisation)\n",
    "2. calculate gradient at this point\n",
    "3. make a scaled step in the opposite direction to the gradient (objective: minimise)\n",
    "4. repeat points 2 and 3 until one of the criteria is met:\n",
    "    1. maximum number of iterations is reached\n",
    "    2. step size is smaller than the tolerance set ad hoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) For arbitrary function $f(x) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ create gradient descent algorithm that tracks the history of iterations. \n",
    "Function shall return the minimum of the mapping. Plot the corresponding points for one-dimensional function $f(x) = x^2 - x + 5$ for big and small learning rate, mark the steps with number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start, gradient, lr = 0.01, max_iter = 1000, tol = 0.01):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x : x**2 - x + 5\n",
    "diffun = lambda x : \n",
    "start = np.random.randint(0, 5)\n",
    "\n",
    "history_small, result_small = gradient_descent(\n",
    "history_big, result_big = gradient_descent("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = \n",
    "yval_s = fun(np.array(history_small))\n",
    "yval_b = fun(np.array(history_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x165a70d4c70>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of the Machine Learning gradient descent, what we need to optimize is a `loss function`, which is a functional of single activation function in the case of perceptron. Then, if we can calculate the gradient $$\\frac{\\partial L}{\\partial w_k},$$ the update of the weights is given by:\n",
    "$$ w_k \\leftarrow w_k - \\eta \\frac{\\partial L}{\\partial w_k}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Linear regression \n",
    "For the situation of linear regression (activation function is $f(x)=x$), implement the previous perceptron that can be taught to adjust its weights. \n",
    "1. set the activation function to be $f(x) = x$,\n",
    "2. implement fit(X, y) method that given training dataset $\\{(\\vec{X}_i, y_i)\\}$ and starting from a random state adjusts the weights to predict the values the best as possible. Use previous gradient descent attitude. \n",
    "3. in order to do so, implement the loss function method to be $L_{MSE}$. '\n",
    "4. for linear regression, gradient of the loss function can be calculated analyticaly - reference the lecture notes.\n",
    "5. use parameter batch if batches are to be calculated in gradient descent\n",
    "6. download the [Fish](https://www.kaggle.com/datasets/aungpyaeap/fish-market/code) dataset from Kaggle and try to fit the linear regression model to find the weight of the fish according to other columns. In order to do so, divide the dataset on 70% of train and 30% of test vectors. Use [train-test-split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or your function.\n",
    "7. try to find the best accuracy according to the $L_{MSE}$ loss function.\n",
    "8. read the documentation of `scikit` library for [Linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), compare results using it to our perceptron. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (87130563.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [269]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_epo(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class PerceptronNew(Perceptron):\n",
    "    # Initialize the bias and the dimension of the perceptron. \n",
    "    # Initialize the weights to be zero (for now at least)\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W,b)\n",
    "        self.__epo = epo\n",
    "        self.__lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "\n",
    "    def get_epo(self):\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    # Create setters for the perceptron\n",
    "    def set_lr(self, lr):\n",
    "\n",
    "    def set_epo(self, epo):\n",
    "\n",
    "\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    \n",
    "    # implement the activation function\n",
    "    def activation_function(self, X):\n",
    "\n",
    "    \n",
    "    # use this loss function to calculate the correctness of the perceptron            \n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        \n",
    "    \n",
    "    # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "    # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "    def fit(self, X, y, randomstate = None, batch = 1):\n",
    "        if randomstate is not None:\n",
    "            self.W = \n",
    "            self.b =\n",
    "        history = []\n",
    "\n",
    "        return history\n",
    "                \n",
    "    def plot_history(self, history):\n",
    "        plt.xlabel('epo')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(history)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bream</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bream</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bream</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bream</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bream</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Smelt</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.0904</td>\n",
       "      <td>1.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Smelt</td>\n",
       "      <td>13.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>1.2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Smelt</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.2770</td>\n",
       "      <td>1.2558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Smelt</td>\n",
       "      <td>19.7</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2.8728</td>\n",
       "      <td>2.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Smelt</td>\n",
       "      <td>19.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.9322</td>\n",
       "      <td>1.8792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0     Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1     Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2     Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3     Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4     Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n",
       "..      ...     ...      ...      ...      ...      ...     ...\n",
       "154   Smelt    12.2     11.5     12.2     13.4   2.0904  1.3936\n",
       "155   Smelt    13.4     11.7     12.4     13.5   2.4300  1.2690\n",
       "156   Smelt    12.2     12.1     13.0     13.8   2.2770  1.2558\n",
       "157   Smelt    19.7     13.2     14.3     15.2   2.8728  2.0672\n",
       "158   Smelt    19.9     13.8     15.0     16.2   2.9322  1.8792\n",
       "\n",
       "[159 rows x 7 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Fish.csv')\n",
    "train_cols = ['Length1','Length2','Length3','Height','Width']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2119516728.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [272]\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_train, X_test, y_train, y_test =\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cols = [df.columns[0]] + list(df.columns[2:]) \n",
    "X_train, X_test, y_train, y_test = \n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Perceptron(np.zeros(5), -1, 500, lr = 1e-6)\n",
    "history = p.fit(X_train[train_cols], y_train, 1, 1)\n",
    "p.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52471.41435966359"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = \n",
    "mse = \n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8489418098056005"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = \n",
    "y_pred_sk = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21358.62577422709"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_sk = \n",
    "mse_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1857361978.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [275]\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_test['Weight_pred'] =\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test['Weight_pred'] = \n",
    "X_test['Weight_pred_sk'] = \n",
    "X_test['Weight'] = \n",
    "X_test['mse'] = \n",
    "X_test['mse_sk'] = \n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare values and syntax with `scikit-learn` [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " array([0, 1, 2, ..., 8, 9, 8]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(return_X_y=True)\n",
    "clf = Perc()\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `What can we do to imporve that?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *6  The Perceptron Algorithm\n",
    "\n",
    "Generate a dataset with 100 numerical pairs ($x$, $y$, $z$). $x^{(i)}, y^{(i)},$ and $z^{(i)}$ should be almost linearly dependent, i.e., $z^{(i)} = a x^{(i)} + b y^{(i)} + c + \\eta_i,\\ \\ i=1,\\ldots,100$,where $a,b,c$ are constants and $\\eta_i$ is random noise.\n",
    "\n",
    "Construct a perceptron which can find the plane ${\\cal }$ best fitted to the points.\n",
    "Use the gradient algorithm to train the system (to solve the regression problem). Check the efficiency of this algorithm for different learning rates and different numbers of iterations (epochs).\n",
    "Rotate the points $x^{(i)}, y^{(i)},$ and $z^{(i)}$ (and plane ${\\cal P}$) in such a way that plane ${\\cal P}$ is perpendicular to the $xy$ plane. Then, use matplotlib to plot the (rotated) points and plane ${\\cal P}$ in a form of a 2D plot with axes $x$ and $y$. Note, that after the rotation plane ${\\cal P}$ will be represented by a line on the $xy$ plane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc8eee665007a42edd269303f37995bd64ca5eac7b16be98d3be7ead3c26aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
