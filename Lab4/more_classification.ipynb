{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Perceptron as Perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>The Perceptron strikes back</center>\n",
    "<center><img src=https://thumbs.gfycat.com/ScalyOpenHydatidtapeworm-max-1mb.gif></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this lab we will extend what we have learnt about classification before. We we will use the skills obtained by building our own perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Come back to the last lab task and recal information about the basic perceptron algorithm. Specifically:\n",
    "- what is the form of the input vector?\n",
    "- what is the body of the perceptron?\n",
    "- what is the `activation function` and what it does?\n",
    "- what is the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the Perceptron class quickly implemented below (or the one made on your own) and make sure that you understand what all the methods do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    # Initialize the bias and the dimension of the perceptron. \n",
    "    def __init__(self, W : np.array,  b = 0):\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "    \n",
    "    ############################## GETTERS ##############################\n",
    "    \n",
    "    def get_N(self):\n",
    "        return self.N\n",
    "    \n",
    "    def get_b(self):\n",
    "        return self.b\n",
    "    \n",
    "    def get_W(self):\n",
    "        return self.W\n",
    "    \n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "        self.W = np.zeros(N)\n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_W(self, W : np.array):\n",
    "        if W.ndim != 1:\n",
    "            print(\"Cannot set such weights -> dimension wrong\")\n",
    "            return\n",
    "        self.N = W.shape[0]\n",
    "        self.W = W\n",
    "    \n",
    "    ############################## GETTERS OVERRIDE ##############################\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.W[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.W[key] = value\n",
    "        \n",
    "    def __getslice(self, i, j):\n",
    "        return self.W[i:j]\n",
    "    \n",
    "    # set the string output of the perceptron \n",
    "    def __str__(self):\n",
    "        return f\"Am a perceptron of N={self.N} dimension{'s' if self.N > 1 else ''} biased with b={self.b}\"    \n",
    "\n",
    "    ############################## OPERATORS OVERRIDE ##############################\n",
    "     \n",
    "    def __mul__(self, other):\n",
    "        return self.activation_function(other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    \n",
    "    '''\n",
    "    Net output is the basic body action of the perceptron. On top of it, the activation function is used.\n",
    "    '''\n",
    "    def net_output(self, X):\n",
    "        return np.dot(X, self.W) + self.b\n",
    "    \n",
    "    '''\n",
    "    Here the activation function is step-like\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, 0.0).reshape((len(X),1))\n",
    "   \n",
    "    '''\n",
    "    Predicts the output of perceptron -> here the class is given   \n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        return self.activation_function(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the extended version of the Perceptron below. What changes are made to it?\n",
    "Please note that gradient descent can be calculated in batches (in case of linear regression):\n",
    "$$w_j\\leftarrow w_j + \\eta \\sum_{i=0}^{n_{batch}} \\delta _i x_j^{(i)}$$\n",
    "\n",
    "The batch size is a `hyperparameter` that defines the number of samples to work through before updating the internal model parameters. Normally batches are randomly selected and this allows to speed up the learning curve without missing the main result of the optimization problem. Nevertheless, one must note that batches cannot be too big (or sometimes even too small), especially when we have only few samples of one class compared to the other ones - unbalanced databases. \n",
    "\n",
    "There are three main ways to define batches, namely:\n",
    "-    Batch Gradient Descent. Batch Size = Size of Training Set\n",
    "-    Stochastic Gradient Descent. Batch Size = 1\n",
    "-    Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set\n",
    "\n",
    "Then we can state that `the batch size is a number of samples processed before the model is updated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLinear(Perceptron):\n",
    "    # Initialize the bias and the dimension of the perceptron -> but not only that\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b)\n",
    "        # how many learning iterations we give\n",
    "        self.epo = epo\n",
    "        # what is our step in the gradient\n",
    "        self.lr = lr\n",
    "    \n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def get_epo(self):\n",
    "        return self.epo\n",
    "    \n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def set_epo(self, epo):\n",
    "        self.epo = epo\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    '''\n",
    "    As you can see the activation function has been changed to linear regression. What does reshape do?\n",
    "    After you figure it out, note that it is the usual way of having the output of the machine learning algorithm. Why?\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return (self.net_output(X)).reshape(-1,1)\n",
    "    \n",
    "    '''\n",
    "    Loss function for the perceptron -> here we use the Mean Square Error\n",
    "    '''           \n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        square = np.square(y_true - y_pred)\n",
    "        return np.mean(square)\n",
    "    \n",
    "    '''\n",
    "    Single step of the gradient, here it is calculated analytically (linear regression)\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        delta_i = (y_true.flatten() - prediction)\n",
    "        suma_w = np.multiply(delta_i[:, np.newaxis], x_true)\n",
    "        suma_b = delta_i         \n",
    "        return suma_b, suma_w\n",
    "    \n",
    "    '''\n",
    "    Fit function allows to obtain the (probably most?) correct weights for the perceptron via the gradient descent algorithm.\n",
    "    '''\n",
    "    def fit(self, X, y, randomstate = None, batch = 1, verbose = False):\n",
    "        \n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "            \n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "        \n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "        # If we want to calculate the gradient in buckets (look for description of the batch)\n",
    "        bucket_num = len(X) // batch\n",
    "        # slice the data onto batches without shuffling (no stochasticity)\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X,bin)\n",
    "                y_slice = slicing(y,bin)\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred = self.predict(X_slice)\n",
    "                \n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # calculate loss\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "                \n",
    "                # update the weights\n",
    "                self.W += np.mean(suma_w, axis = 0) * self.lr\n",
    "                self.b += np.mean(suma_b, axis = 0) * self.lr \n",
    "            # calculate average loss\n",
    "            loss/=bucket_num\n",
    "            if verbose:\n",
    "                print(f'epo:{epo}->loss={loss}')        \n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()\n",
    "    '''\n",
    "    Basic history plot\n",
    "    '''        \n",
    "    def plot_history(self, history, ax = None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('epo')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.plot(history)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Test the linear Perceptron on linearized data with normal noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(0, 100, 1)\n",
    "y = 3 * X + 5 + np.random.normal(0, 1e1, size = len(X))\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train = \n",
    "y_train = \n",
    "X_test = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = \n",
    "\n",
    "history = lin_reg.fit(X=X_train, y=y_train, randomstate=, batch=)\n",
    "lin_reg.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color=\"blue\", s=10, label=\"train_data\")\n",
    "plt.scatter(X_test, y_test, color=\"green\", s=10, label=\"test_data\");\n",
    "plt.scatter(X_test, lin_reg.activation_function(X_test).reshape(-1, 1), color=\"red\", s=10, label=\"pred_data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Taking the inspiration from the work above work out the procedure to create a Perceptron that is able to learn basic classification. \n",
    "Recall the information from the lecture. Use the MSE loss function, which for binary classification is of much simpler form. Use the loss function that has been calculated analytically for binary labels $\\{-1,1\\}$:\n",
    "$$\\vec{W} \\leftarrow \\vec{W} + \\eta y_i (1-y_i \\hat{y}_i)\\vec{X_i}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronBinary(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "        \n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Loss function for the perceptron -> here we use knowledge that the classes can be either {-1, 1}\n",
    "    '''           \n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Single step of the gradient, here it is calculatable analytically (linear regression). It shall return a sum for bias and sum for weights\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        \n",
    "        suma_w = \n",
    "        suma_b =       \n",
    "        return suma_b, suma_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall this:\n",
    "\n",
    "\"\n",
    "\n",
    "Set the appropriate weights for the perceptron to reproduce this table:\n",
    "\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "    \\text{Training example} & x_1 & x_2 & \\text{Classification} \\\\\n",
    "    A&0&1&-1\\\\\n",
    "    B&2&0&-1\\\\\n",
    "    C&1&1&1\\\\\n",
    "    D&2&3&1\n",
    "\\end{array}\n",
    "\n",
    "\"\n",
    "\n",
    "#### d) Use the Perceptron above to check, whether you were right about the weights during the previous lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PerceptronBinary(np.random.random(2), np.random.random(), 100, 1e-1)\n",
    "x_train = [[0.,1.], [2.,0.], [1.,1.], [2.,3.]]\n",
    "y_train = [-1., -1., 1.,1.]\n",
    "for d in x_train:\n",
    "    plt.scatter(d[0],d[1])\n",
    "plt.legend(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = a.fit()\n",
    "a.plot_history(history)\n",
    "a.predict(x_train), a.get_W()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when the data is not linearly seperable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a vector find a vector orthogonal to it\n",
    "'''\n",
    "def find_orthogonal_vector(x):\n",
    "    x_p = np.random.random(len(x))\n",
    "    x_p -= x_p.dot(x) * x / np.linalg.norm(x)**2.0\n",
    "    return x_p\n",
    "\n",
    "'''\n",
    "Find vector from two points\n",
    "'''\n",
    "def find_plane_from_ort(perp1, perp2):\n",
    "    # a * x1 + b = y1\n",
    "    # a * x2 + b = y2\n",
    "    # -> a * (x1-x2) = (y1-y2) -> a = (y1-y2)/(x1-x2)\n",
    "    a = (perp1[1] - perp2[1])/(perp1[0]-perp2[0])\n",
    "    b = perp1[1] - a * perp1[0] \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Take 50 randomly initialized points in 2D -> $r=[x=rand(), y=rand()]$ such that $x,y \\in [-1,1]$. \n",
    "For each of them assign a class such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \n",
    "\\left \\{\n",
    "   \\begin{array}{lr}\n",
    "       1 & y \\geq 3 * x - 1 \\\\\n",
    "       -1 &\\text{otherwise}\n",
    "   \\end{array}\n",
    "\\right .\n",
    "\\end{equation}\n",
    "\n",
    "Randomly initialize vector $\\vec{W}$ of the perceptron. \n",
    "\n",
    "Train the perceptron to fit the data. Use 50 epochs. At each 10th epoch plot the vector perpendicular to $\\vec{W}$ Use the functions above to find the perpendicular vector and plane (line) in 2D. What can we tell about the learning algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize = (5,5))\n",
    "x_train = 2.0*np.random.random((50, 2)) - 1.0\n",
    "y_train = \n",
    "\n",
    "# set the x_range to plot the real line:\n",
    "x=np.arange(-1, 1, 1e-2)\n",
    "\n",
    "# set the classes for the plot\n",
    "colors = ['red' if y > 0 else 'blue' for y in y_train]\n",
    "labels = ['1' if y > 0 else '-1' for y in y_train]\n",
    "\n",
    "ax[0].plot(x, 3*x-1, linestyle=':', label = 'true')\n",
    "ax[0].scatter(x_train[:,0], x_train[:,1], c = colors)\n",
    "ax[0].set_xlim(-1,1)\n",
    "ax[0].set_ylim(-1,1)\n",
    "\n",
    "# set the initial vector W to be two-dimensional random\n",
    "W = \n",
    "# find two orthogonal vectors and then the hyperplane corresponding to them\n",
    "perp1 = \n",
    "perp2 = \n",
    "plane = \n",
    "\n",
    "p=PerceptronBinary(W, np.random.random(), epoch=1, lr=1e-3)\n",
    "\n",
    "epo = 101\n",
    "# take the snapshot at each 50th step\n",
    "modulo = 50\n",
    "history=[]\n",
    "for i in range(epo): \n",
    "    if i % modulo == 0:\n",
    "        perp1 = \n",
    "        perp2 = \n",
    "        plane = \n",
    "        ax[0].scatter(x, plane[0] * x + plane[1], label = i, s=2)\n",
    "        ax[0].plot([0.0, W[0]],[0.0,W[1]],c='black')\n",
    "        \n",
    "    history = history + list(p.fit(x_train, y_train))\n",
    "    W = p.get_W()\n",
    "    \n",
    "ax[0].legend()\n",
    "p.plot_history(history, ax[1])\n",
    "(p.predict(x_train).flatten()- y_train) != 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [DISCLAIMER] Note that the plot of the decision bound is in fact here just conceptual as $W_0=b$ is also sought with the gradient descent and is not used for the plot. \n",
    "\n",
    "We can see that the perceptron is a basic concept that can be treated as a linear separation classifier. The linearization depends though on the way the data is treated. This means that changing the activation function can change the way Perceptron is classifying things either by linear regression or plane separation. Therefore, vector perpendicular to the Perceptron body weights gives us a linear decision boundary in a given dimension:\n",
    "\n",
    "$$ \\vec{X} \\cdot \\vec{W} = 0.$$\n",
    "\n",
    "In binary classification then we can be clever and consolidate the output onto just one equation:\n",
    "\n",
    "$$ -y^i \\vec{X^i}\\cdot \\vec{W} < 0.$$\n",
    "\n",
    "We can introduce the idea of another function commonly used in Machine Learning, inputing in into the cost function. Namely, one can use the function:\n",
    "\n",
    "$$ g_i(\\vec{W}) = \\max{(0, -y^i \\vec{X^i}\\cdot \\vec{W})}. $$ \n",
    "\n",
    "If the result is classified correctly, the function is always nonnegative. (SHOW WHY?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a cost function is known as [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (rectified linear unit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(-1, 1, 1e-2)\n",
    "plt.plot(x, x*(x>0))\n",
    "plt.title('ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\max{(0,x)}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case the total cost function can be written as:\n",
    "$$ L = \\frac{1}{N}\\sum _i g_i(\\vec{W}).$$\n",
    "Sometimes such a cost function is called a hinge loss. It is always convex (why is it important?) but we cannot use a Newton's method for it as it is not continoius at $x=0$! \n",
    "\n",
    "##### [TIP] Sometimes to fix the gradient in a function one uses smothened versions of ReLU with different parameters to smoothen them. Their usage is discussed broadly in Machine Learning community.\n",
    "\n",
    "##### [DISCLAIMER] Keep in mind that here ReLU is used as a loss function but mostly it is discused in terms of being the activation function. Nevertheless, it turns out that for a single perceptron it means the same. \n",
    "\n",
    "One of the examples that can be used instead is a function called Softmax:\n",
    "\n",
    "$$ g(x) = soft(0, x) = \\log(1+e^s)\\rightarrow g^i(\\vec{W}) = soft(0, -y^i\\vec{X}^i\\cdot \\vec{W})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Calculate the gradient $\\frac{\\partial}{\\partial W_k}$ of the newly created loss function with the softmax activation.\n",
    "$$ L_i = \\log(1+e^{ -y^i\\vec{X}^i\\cdot \\vec{W}}). $$\n",
    "Change perceptron and use it for classification of the random data again. Which activation performs better? Try using batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSoftmax(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "        \n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "\n",
    "    \n",
    "         \n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "\n",
    "        \n",
    "    '''\n",
    "    Single step of the gradient, here it is calculatable analytically (linear regression)\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "\n",
    "        suma_w = \n",
    "        suma_b =          \n",
    "        return suma_b, suma_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it by doing the same plot as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize = (5,5))\n",
    "x_train = 2.0*np.random.random((50, 2)) - 1.0\n",
    "y_train = \n",
    "\n",
    "# set the x_range to plot the real line:\n",
    "x=np.arange(-1, 1, 1e-2)\n",
    "\n",
    "# set the classes for the plot\n",
    "colors = ['red' if y > 0 else 'blue' for y in y_train]\n",
    "labels = ['1' if y > 0 else '-1' for y in y_train]\n",
    "ax[0].plot(x, 3*x-1, linestyle=':', label = 'true')\n",
    "ax[0].scatter(x_train[:,0], x_train[:,1], c = colors)\n",
    "ax[0].set_xlim(-1,1)\n",
    "ax[0].set_ylim(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method. \n",
    "For smaller datasets it is sometimes usefull to use the Newton's method to find the function minumum with faster convergence. Suppose we have some function:\n",
    "$$ f : \\mathbb{R} \\rightarrow \\mathbb{R} .$$\n",
    "\n",
    "We try to find a value where f(x) = 0 (or as close as possible). For this we set some initial point $x_0$ and calculate $f(x_0)$. If $x_0$ is near some real point close to 0 - $\\alpha$, then the tangent line at $x_0$ crosses the x-axis closer and closer to $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_and_b_and0(prev_x, prev_y, diff):\n",
    "    # y_0 = a_0 * x_0 + b_0 -> b_0 = y_0 - a_0*x_0\n",
    "    a = diff(prev_x)\n",
    "    b = prev_y - a * prev_x \n",
    "    zero = - b / a\n",
    "    return a, b, zero\n",
    "\n",
    "x=np.arange(-1,1,1e-2)\n",
    "f = lambda x : np.square(x)\n",
    "df = lambda x : 2 * x\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.axvline(0.0, c='black')\n",
    "plt.axhline(0.0, c='black')\n",
    "\n",
    "x_0 = 0.75\n",
    "y_0 = f(x_0)\n",
    "plt.scatter(x_0, y_0, c='red')\n",
    "\n",
    "# find first tangent line\n",
    "a_0, b_0, x_1 = find_a_and_b_and0(x_0, y_0, df)\n",
    "y_1 = f(x_1)\n",
    "plt.plot(x[len(x)//2:], a_0 * x[len(x)//2:] + b_0)\n",
    "plt.scatter(x_1, y_1, c='red')\n",
    "\n",
    "# find second tangent line\n",
    "a_1, b_1, x_2 = find_a_and_b_and0(x_1, y_1, df)\n",
    "y_2 = f(x_2)\n",
    "plt.plot(x[len(x)//2:], a_1 * x[len(x)//2:] + b_1)\n",
    "plt.scatter(x_2, y_2, c='red')\n",
    "\n",
    "# find third tangent line\n",
    "a_2, b_2, x_3 = find_a_and_b_and0(x_2, y_2, df)\n",
    "y_3 = f(x_3)\n",
    "plt.plot(x[len(x)//2:], a_2 * x[len(x)//2:] + b_2)\n",
    "plt.scatter(x_3, y_3, c='red')\n",
    "\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extrema of the loss function correspon to points where its first derivative is zero so we need to use the same algorithm by simply calculating the second derviatives of the function. If $\\vec{W}$ is the vector of weights, we need to generalize it to multidimensional system and use [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) method.\n",
    "\n",
    "$$ \\vec{W} \\leftarrow W - H^{-1} \\nabla _{\\vec{W}} \\vec{L}(\\vec{W}),$$\n",
    "\n",
    "where $\\nabla _{\\vec{W}} \\vec{L}(\\vec{W})$ is the vector of partial derivatives of the cost function with respect to $W$ and $H$ os a $n\\times n$ matrix ($n+1 \\times n+1$) with bias called the Hessian such that:\n",
    "\n",
    "$$ H_{ij} = \\frac{\\partial ^2 L(\\vec{W})}{\\partial w_i \\partial w_j}.$$\n",
    "\n",
    "Newton’s method typically enjoys faster convergence than (batch) gradient descent and requires many fewer iterations to get very close to the maximum. One iteration of Newton’s can, however, be more expensive than one iteration of gradient descent, since it requires finding and inverting a d × d Hessian; but so long as d is not too large, it is usually much faster overall. Application of Newton’s method to maximize the logistic regression log likelihood function is also called Fisher scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Add the method to calculate Newton-Raphson algorithm in the previous Perceptron. In order to do this, calculate the derviative:\n",
    "\n",
    "$$ H_{ij} = \\frac{\\partial ^2 }{\\partial w_i \\partial w_j} \\log(1+e^{ -y^i\\vec{X}^i\\cdot \\vec{W}}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSoftmax(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "        \n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "\n",
    "    \n",
    "         \n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "\n",
    "        \n",
    "    '''\n",
    "    Single step of the gradient, here it is calculatable analytically (linear regression)\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "\n",
    "    \n",
    "    def hessian(self, x_true, y_true, y_pred):\n",
    "\n",
    "    \n",
    "    def newton_rap(self, X, y, randomstate = None, verbose = False):        \n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "            \n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "        \n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "\n",
    "        # iterate epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h) Use the following data to test the algorithm for Newton-Rhapson\n",
    "Use 5 epochs and $\\vec{W} = [1],b=1$ as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-0.3, -0.1, 0.3, 0.5, 1.0, 1.3, 4.5, 3.5]).reshape(-1,1)\n",
    "Y = np.array([-1,-1,-1, -1, 1,1,1,1],dtype=np.float64).reshape(-1,1)\n",
    "plt.plot(X,Y)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=PerceptronSoftmax(\n",
    "history=p.newton_rap(X, Y)\n",
    "p.plot_history(history)\n",
    "p.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Do the same for vector $\\vec{W}=[4.0], b=-10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=PerceptronSoftmax(np.array([4.]),-10., epo = 100)\n",
    "history=p.newton_rap(\n",
    "p.plot_history(history)\n",
    "p.predict(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### j) Look at an example below and analyse it. What happens with the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "bs = np.arange(-10, 10, 0.5)\n",
    "ws = np.arange(-10, 10, 0.5)\n",
    "for b in bs:\n",
    "    for w in ws:\n",
    "        p=PerceptronSoftmax(np.array([w]),b)\n",
    "        pred = p.predict(X)\n",
    "        loss=p.loss(Y, pred)\n",
    "        losses.append(np.mean(loss))\n",
    "        \n",
    "p=PerceptronSoftmax(np.array([1.0]),-1, 1, lr=1e-3)\n",
    "\n",
    "size = len(ws)\n",
    "c=plt.imshow(np.array(losses).reshape((size,size)))\n",
    "plt.colorbar(c)\n",
    "\n",
    "bs = []\n",
    "ws = []\n",
    "for i in range(1000):\n",
    "    p.newton_rap(X, Y)\n",
    "    b=p.get_b()\n",
    "    w=p.get_W()[0]\n",
    "    bs.append(b + size/2)\n",
    "    ws.append(w + size/2)\n",
    "    \n",
    "plt.plot(ws, bs , c = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can observe from here is that the minimum of the function is constant for the weights going to infinity! How can we prevent such situation? ---> One can prevent this by using small learning rate or batches with the early stopping. \n",
    "\n",
    "##### [DISCLAIMER] Early stopping is a method that tracks the history of the loss function and twhen change after small number of eochs is lower than some threshold, or is netagtive for longer period, the learning procedure is stopped.\n",
    "\n",
    "\n",
    "Another approach is to control the magnitude of the weights via `REGULARIZATION`. Normally we add some kind of norm to the loss function that we need to minimize. Usually, we take the length of the vector or a second order norm $||W|| or ||W||_2$, which are called $L_1$ and $L_2$ regularizations accordingly. Then, our loss function reassembles the behavior of Lagrange multipliers and looks like:\n",
    "\n",
    "$$ L'= L+\\lambda||W||_2,$$\n",
    "\n",
    "where $\\lambda$ is a small regularization parameter.\n",
    "##### [DISCLAIMER] Using regularization is also useful for phenomena called [OVERFITTING](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "#### k) Implement the regularization to the method. Use the loss that is suitable for you. You have to calculate the gradient of the newly created loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is yet another method of classification, which in principle can be accorded to a most common ML algorithms - the classification. It can be used for problems like spam classifiers, baseline primary classifiers, disease recognition etc. There are many multi-class versions of logistic regression. \n",
    "\n",
    "It basically uses the same tools as our Perceptron but changes the way activation function and loss is treated. `Recall the information about the logistic regression from the lecture`.\n",
    "\n",
    "Logistic regression uses [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function:\n",
    "\n",
    "$$ p(\\vec{W}, \\vec{X}) = \\frac{1}{1 + e^{-W\\cdot X}}$$\n",
    "\n",
    "It is used as an estimator of probability that a given class belongs to some label via maximizing the likelyhood instead of minimizing the distance. The probability equation that wew want to maximize is given by:\n",
    "$$P(\\vec{y}|\\vec{W};\\vec{X})= \\sum _ i  p^{y^{i}}(1-p)^{1-y^{i}}$$\n",
    "\n",
    "Then, it is easier to apply the sum of logarithms of such probabilities, therefore, after manipulations (from the lecture though), what we obtain is:\n",
    "$$ L = -\\sum _i [y^i\\log p_i + (1-y^i)\\log(1-p_i)]$$\n",
    "where minus has been taken because maximizing likelyhood now coresponds to minimizing the loss and the gradient is easily obtained:\n",
    "$$L = [y-\\sigma (W\\cdot X)]X, $$\n",
    "where $\\sigma (x) $ is the sigmoid function. Then we follow exactly the same procedure as before to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k) Create the logistic regression algorithm using the information from above. Implement Newton-Rhapson for it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l) Use LogisticRegression from sklearn and predict the linearly and nonlinearly separable data in 1D, just as before. Compare your model to the model used here. \n",
    "\n",
    "#### For the throughout guide of sklearn Logistic Regression one can follow this [website](https://realpython.com/logistic-regression-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://cdn.vox-cdn.com/uploads/chorus_asset/file/13468746/giphy.gif></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m) Use the inbuilt Logistic Regressor from sklearn to test the classsification of handwritten digits database from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc8eee665007a42edd269303f37995bd64ca5eac7b16be98d3be7ead3c26aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
