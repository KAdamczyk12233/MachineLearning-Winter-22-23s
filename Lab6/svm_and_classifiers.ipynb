{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svm menace\n",
    "<center><img src=https://media.tenor.com/2zf2ujSCYHkAAAAC/palpatine-trained-well.gif></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is the continuation on Support Vector Machnies per se. Last time we have seen how the creation of the `street` with maximal width corresponds to minimizing the norm of weights vector $\\vec{W}$ with constraints:\n",
    "$$y_i (\\vec{W} \\cdot \\vec{X_i} + b) \\geq 1 \\quad \\text{for each sample}.$$\n",
    "\n",
    "The constraints are then introduced to the Lagrange function:\n",
    "\n",
    "$$ L(\\vec{W}, b, \\vec{\\alpha}) = \\frac{1}{2} ||\\vec{W}||^2 - \\sum _i \\alpha _i \\left[y_i (\\vec{W}\\cdot \\vec{X} + b)-1\\right], $$\n",
    "which can be numericaly maximized and the extremum can be found. This solves the problem of classification, whether the data is well separated in a linear manner. Although useful, this is not the perfect solution to any of the potential problems.\n",
    "\n",
    "Namely, what if we are not so certain about the prediction and/or the data is not perfectly separable? We can use such though by introducing a `penalty` term to the classification:\n",
    "\n",
    "$$ \\min {\\vec{W},b} \\frac{1}{2} ||\\vec{W}||^2 + C\\sum _i \\xi _i, $$\n",
    "\n",
    "where $C$ is a regularization term that works in a similar manner to a learning rate.  It does mean that we allow some points to be located within the margins or even on the wrong side of the decision boundary. Surly, the amount of penalty should also be minimized.\n",
    "<center><img src=slack_1.png></center>\n",
    "\n",
    "[`!DISCLAIMER`] A regularization parameter is commonly used to prevent overfitting. We have discussed it, when we create our Perceptron. There exist plenty of possible regularization and `slack variables` $\\xi$ are only one possiblity. Parameter $C$ in front of the term tells us how strongly we want to penalize the model for wrong predictions.\n",
    "\n",
    "Now, the constraint changes slightly to be:\n",
    "$$y_i (\\vec{W} \\cdot \\vec{X_i} + b) \\geq 1 - \\xi _i \\quad \\text{for each sample}.$$\n",
    "\n",
    "But what is $\\xi _i$ exactly? It is a term that whenever we don't follow the hard-margin problem $y_i (\\vec{W} \\cdot \\vec{X_i} + b)\\geq 1$, it introduces the value $\\xi_i \\equiv 1-y_i(\\vec{W} \\cdot \\vec{X_i} + b)$. Otherwise, it does not add any penalty and therefore $\\xi _i = 0$. Therefore, one can say:\n",
    "\n",
    "$$ \\xi _i = \\max [0, 1-y_i(\\vec{W} \\cdot \\vec{X_i} + b)].$$ \n",
    "\n",
    "\n",
    "You can visualize how the slack variables work and play with them under this [link](https://greitemann.dev/svm-demo). In order to do so, create linearly separable data and start to add one class points at wrong site of the street. You will see that support vectors are now defined inside the street and some of them stay missclassified.\n",
    "\n",
    "<center><img src=svm_slack.png></center>\n",
    "\n",
    "Let us consider this. As previously, let's start with the simple cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) How does the constraint in the Lagrangian look in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) How does the Lagrangian look in this case? Remember now that it introduces two new variables $\\vec{\\xi}$ and $\\vec{\\eta}$ (for slack variables minimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the derivatives and demand them to be zero, we get:\n",
    "$$ \\frac{\\partial L}{\\partial \\vec{W}} = 0 \\rightarrow \\vec{W} = \\sum _i \\alpha _i y_i \\vec{X} _i,$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = 0 \\rightarrow \\sum _i \\alpha _i y_i = 0,$$\n",
    "$$ \\frac{\\partial L}{\\partial \\xi _i} = 0 \\rightarrow \\eta _i = C - \\alpha _i.$$\n",
    "\n",
    "This leads to the same form of the Lagrangian as a function of $\\vec{\\alpha}$ as we had before:\n",
    "\n",
    "$$ L(\\vec{\\alpha}) = \\sum _i \\alpha _i - \\frac{1}{2} \\sum _{i,j} \\alpha _i \\alpha _j y_i y_j \\vec{X}_i\\cdot \\vec{X} _j, $$\n",
    "\n",
    "`[!Disclaimer] The support vectors are defined whenever ` $\\alpha _i \\neq 0$.\n",
    "#### c) Derive the constraint on $\\alpha$ knowing than $\\eta _i$ is a Lagrange multiplier and $\\eta _i \\geq 0$ for each sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Take the data below and numerically find the solution of the Lagrange problem. \n",
    "- Plot the data and plot the corresponding support vectors. \n",
    "- Plot the weights vector and the street. \n",
    "- Calculate the width of the street. \n",
    "- Find the slack variables for the outliers. Plot the distribution of the slack variables.\n",
    "You can use the optimizy method from scipy: [optimization minimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) as before. Define Lagrange function to be $-L(\\vec{W})$ in order to find the maximum and use the constraint on the Width of the street to find the optimal values. Experiment with $C$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.xlim(-1.0, 1.0)\n",
    "plt.ylim(-1.0, 1.0)\n",
    "\n",
    "# create linearly separable data\n",
    "n_samples = 20\n",
    "# create random points from that are defined as y > x to be assign with value -1\n",
    "X1 = [-1.0 + 2*np.random.random() for i in range(n_samples)]\n",
    "X1 = [np.array([i, i + np.random.random()]) for i in X1]\n",
    "Y1 = [-1 for i in range(len(X1))]\n",
    "# create random points from that are defined as y < x to be assign with value 1\n",
    "X2 = [-1.0 + 2*np.random.random() for i in range(n_samples)]\n",
    "X2 = [np.array([i, i - np.random.random()]) for i in X2]\n",
    "Y2 = [1 for i in range(len(X2))]\n",
    "\n",
    "# add some outliers (the points y > x have the value 1 and y < x -1)\n",
    "n_outliers = 2\n",
    "X1_o = [-1.0 + 2*np.random.random() for i in range(n_outliers)]\n",
    "X1_o = [np.array([i, i - np.random.random()]) for i in X1_o]\n",
    "Y1_o = [-1 for i in range(len(X1_o))]\n",
    "X2_o = [-1.0 + 2*np.random.random() for i in range(n_outliers)]\n",
    "X2_o = [np.array([i, i + np.random.random()]) for i in X2_o]\n",
    "Y2_o = [-1 for i in range(len(X2_o))]\n",
    "\n",
    "# plot the values\n",
    "plt.plot(np.arange(-1.0, 1.0, 1e-3), np.arange(-1.0, 1.0, 1e-3), linestyle = '--')\n",
    "plt.scatter(np.array(X1)[:,0], np.array(X1)[:,1], color = 'red')\n",
    "plt.scatter(np.array(X1_o)[:,0], np.array(X1_o)[:,1], color = 'red')\n",
    "plt.scatter(np.array(X2)[:,0], np.array(X2)[:,1], color = 'blue')\n",
    "plt.scatter(np.array(X2_o)[:,0], np.array(X2_o)[:,1], color = 'blue')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# concatenate all the data\n",
    "X_train = X1 + X2 + X1_o + X2_o\n",
    "Y_train = Y1 + Y2 + Y1_o + Y2_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note that the Lagrange function has minus at the beginning in order to use the scipy.optimize.minimize function.\n",
    "'''\n",
    "# product y_i * y_j\n",
    "Y_Y_T = np.zeros((len(Y_train), len(Y_train)))\n",
    "# dot product x_i * x_j\n",
    "DOTS = np.zeros((len(Y_train), len(Y_train)))\n",
    "\n",
    "# fill the values that we can calculate beforehand\n",
    "for i,xi in enumerate(X_train):\n",
    "    for j,xj in enumerate(X_train):\n",
    "        DOTS[i,j] = \n",
    "        Y_Y_T[i,j] = \n",
    "\n",
    "def Lagrange(alphas : np.ndarray):\n",
    "    sum_a = # sum alphas -> first term in the lagrangian\n",
    "    sum_x = 0\n",
    "    for i,xi in enumerate(alphas):\n",
    "        for j,xj in enumerate(alphas):\n",
    "            sum_x += # the rest of the lagrangian\n",
    "    return # return -Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the constraint with [scipy.optimize.LinearConstraint](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constraint\n",
    "C = \n",
    "constraint = # \\sum _i \\alpha i * y_i = 0\n",
    "constraints = []\n",
    "for i in range(len(Y_train)):\n",
    "    a = np.zeros_like(Y_train)\n",
    "    a[i] = # ???\n",
    "    # append constraint that $alpha i is between 0 and C\n",
    "    constraints.append(optimize.LinearConstraint(a, 0.0, C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimize and get the values of $\\vec{\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = optimize.minimize(Lagrange, x0 = ????????, constraints=[constraint] + constraints)\n",
    "alphas = a['x']\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the value for b for any of the Support Vectors $\\vec{X}$. In order to find support vectors, find $\\alpha _i$ different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find support vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the corresponding value for $\\vec{W}$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((2))\n",
    "for i, s in enumerate(X_train):\n",
    "    # go through points to find W\n",
    "    ?????\n",
    "print(\"W =\", W)\n",
    "\n",
    "different_from_one = 1e13\n",
    "\n",
    "b = 0\n",
    "# find b -> it corresponds to the situation, where Y_i * (np.dot(W, X_i) + b) = 1\n",
    "for i in range(len(X_train)):\n",
    "    ???\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find two support vectors with different values of Y $(\\pm 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sone = None\n",
    "Smone = None\n",
    "\n",
    "\n",
    "for i, s in enumerate(X_train):\n",
    "    output_dot = np.dot(W, s) + b\n",
    "    ???\n",
    "    ???\n",
    "    \n",
    "    #if Sone is not None and Smone is not None and np.random.random() < 0.2:\n",
    "    #    break\n",
    "print(Sone_close, Smone_close)\n",
    "Sone, Smone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the model predictions vs real class labels. Use the metric from [scikit-learn accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for x in X_train:\n",
    " ### append scores of prediction?\n",
    " # How do we predict? Check the lecture SVM2\n",
    "accuracy_score(y_pred, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# define the calculated points\n",
    "c = -b/W[1]\n",
    "a = -W[0]/W[1]\n",
    "print(\"Crossing point c=\",c,\"Tangent coefficient a= \", a)\n",
    "\n",
    "# perpendicular function to W - this can be defined for any support vectors as well\n",
    "# decision line\n",
    "fp=lambda x: a * x + c\n",
    "\n",
    "# a function for parallel function to W (contains the vector W)\n",
    "f=lambda x: -1.0/a * x + c\n",
    "\n",
    "# set xrange\n",
    "x = np.arange(-2, 2, 0.01)\n",
    "\n",
    "# plot decision function\n",
    "\n",
    "\n",
    "\n",
    "# plot W vector\n",
    "norm = np.sqrt(np.sum(np.square(W)))\n",
    "\n",
    "# plot the support vectors\n",
    "\n",
    "\n",
    "# scatter the training points with correct colors\n",
    "\n",
    "# plot the prediction with different colors\n",
    "for j, g in enumerate(X_train):\n",
    "\n",
    "# set title with the accuracy   \n",
    "ax.set_title(f\"Accuracy={accuracy_score(y_pred, Y_train):.4f}\")\n",
    "\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_xlabel('$x_1$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the soft margin accuracy with different constants $C$ to hard margin solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even if we can in principle achieve some better results for SVM that separates the classes linearly, we can rarely separate the data perfectly. There exists a trick that allows to do so. This is called a `kernel trick` that enables us to transform the data in $\\mathcal{X}$ space to new space $\\mathcal{Y}$ that is normally more dimensional.\n",
    "\n",
    "For example, if data $\\vec{X} = (x_1, x_2) \\in \\mathcal{R}^2$ is not linearly separatable, we can transform it in a following manner $\\mathcal{X}=\\mathcal{R}^2 \\rightarrow \\mathcal{Y}= \\mathcal{R}^3$:\n",
    "\n",
    "$$ vec{X} = (x_1, x_2) \\rightarrow \\vec{Z} = (z_1, z_2, z_3) = (x_1^2, \\sqrt(2) x_1x_2, x_2^2),$$\n",
    "\n",
    "which allows for new decision boundary $\\vec{W} \\cdot \\vec{Z} + b = 0$, such that:\n",
    "\n",
    "$$ w_1 \\cdot x_1^2 + w_2 \\sqrt(2) x_1x_2 + w_3 x_2^2 + b = 0.$$\n",
    "\n",
    "\n",
    "<center><img src=svm_kernel1.png></center>\n",
    "\n",
    "This is sometimes useful, as introduction of new dimension and/or mapping, can make the data linearly separable in the new space. We can do it, by transforming each of the training vectors $\\vec{X}_i \\rightarrow \\Phi(\\vec{X}_i)$ for some mapping function $\\phi (\\vec{X}) : \\mathcal{X} \\rightarrow \\mathcal{Y}$. This is unfortunatelly highly complicated and computationaly expensive. Yet, there exists some mappings that are different from the other, for which the dot product (that is necessary for our Lagrange function) is transformed the same way as a single vector. \n",
    "\n",
    "$$ K(\\vec{X}, \\vec{Y}) = \\phi(\\vec{X}) \\cdot \\phi(\\vec{Y}). $$\n",
    "\n",
    "`Do you recognize the property from mathematics?`\n",
    "\n",
    "#### c) Find out what transformations of kernels keep us with the same property of homomorphism.\n",
    "This is shown in lecture notes SVM4.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Visualize how different kernels work and play with them under this [link](https://greitemann.dev/svm-demo). In order to do so, create linearly nonseparable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) For a given data use the SVM from [scikit-learn](https://scikit-learn.org/stable/modules/svm.html). Use different kernels and compare the results.\n",
    "- plot the support vectors\n",
    "- test the accuracy for different kernels\n",
    "- check other parameters in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-4,4)\n",
    "\n",
    "# elipse\n",
    "decision_function = lambda x, a, b: b*np.sqrt(1 - x**2/a**2)\n",
    "a = 0.707\n",
    "b = 3.14\n",
    "n = 500\n",
    "\n",
    "X = -2.0 + 2 * 2 * np.random.random(n)\n",
    "Y = -4.0 + 2 * 4 * np.random.random(n)\n",
    "X_m1 = []\n",
    "X_1 = []\n",
    "Y_train = []\n",
    "X_train = []\n",
    "\n",
    "# add classes\n",
    "for i, x in enumerate(X):\n",
    "    if x**2 / a**2 + Y[i]**2 / b**2 > 1:\n",
    "        X_1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        X_m1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(0) \n",
    "    X_train.append(np.array([x, Y[i]]))\n",
    "    \n",
    "n = 20\n",
    "X = -2.0 + 2 * 2 * np.random.random(n)\n",
    "Y = -4.0 + 2 * 4 * np.random.random(n)       \n",
    "# add outliers\n",
    "for i, x in enumerate(X):\n",
    "    if x**2 / a**2 + Y[i]**2 / b**2 > 1:\n",
    "        X_m1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(0)\n",
    "    else:\n",
    "        X_1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(1)\n",
    "    X_train.append(np.array([x, Y[i]]))\n",
    "X_1 = np.array(X_1)\n",
    "X_m1 = np.array(X_m1)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "\n",
    "X = np.arange(-2.5, 2.5, 1e-4)\n",
    "plt.plot(X, decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "plt.plot(X, -decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "\n",
    "plt.scatter(X_m1[:,0], X_m1[:,1], color = 'red', label = '0')\n",
    "plt.scatter(X_1[:,0], X_1[:,1], color = 'blue', label = '1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# define different kernels\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# choose random or choose by hand\n",
    "kernel = \n",
    "\n",
    "print(\"Kernel = \", kernel)\n",
    "\n",
    "clf = \n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the predictions and see how it goes :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "X_test = np.array([np.array([-2.0 + 2 * 2 * np.random.random(), -4.0 + 2 * 4 * np.random.random()]) for i in range(n)])\n",
    "Y_pred = # predict the data\n",
    "colors = ['red' if i == 0 else 'blue' for i in Y_pred]\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-4,4)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Prediction for {kernel} kernel')\n",
    "\n",
    "# plot the elipse\n",
    "X = np.arange(-2.5, 2.5, 1e-4)\n",
    "plt.plot(X, decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "plt.plot(X, -decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "\n",
    "# scatter points\n",
    "for i in range(n):\n",
    "    if colors[i] == 'red':\n",
    "        zero = plt.scatter(X_test[i][0], X_test[i][1], color = colors[i], label = '0')\n",
    "    else:\n",
    "        one = plt.scatter(X_test[i][0], X_test[i][1], color = colors[i], label = '1')\n",
    "\n",
    "plt.legend(handles = [zero, one])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Test the accuracy as previously. Use the $X_{train}$ and $Y_{train}$ data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://y.yarn.co/b5e4ac74-503d-48d7-8334-de490ec2872b_text.gif></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want, you can implement a SVM class for up to `60 additional points` (or alternatively, potential 5.5 grade with that many points). It can be based on the scikit-learn class, yet must be done by yourself as a whole and sent by the end of `last tutorials`. You shall show how it works for different types of data and implement kernel trick, soft margin and hard margin. It shall also compare clustering algorithms shown during the lecture to `your SVM`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc8eee665007a42edd269303f37995bd64ca5eac7b16be98d3be7ead3c26aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
